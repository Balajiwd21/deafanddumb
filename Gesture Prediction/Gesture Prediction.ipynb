{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello' 'Love You' 'Understand' 'Thanks' 'Some' 'Home' 'name' 'my' 'how'\n",
      " 'Sorry' 'Help me' 'Yes' 'No' 'eat' 'friend']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SURYA S\\gesture\\Gesture Prediction\\Gesture Prediction.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SURYA%20S/gesture/Gesture%20Prediction/Gesture%20Prediction.ipynb#ch0000001?line=104'>105</a>\u001b[0m cv2\u001b[39m.\u001b[39mputText(frame, className, (\u001b[39m10\u001b[39m, \u001b[39m50\u001b[39m), cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX, \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SURYA%20S/gesture/Gesture%20Prediction/Gesture%20Prediction.ipynb#ch0000001?line=105'>106</a>\u001b[0m                \u001b[39m1\u001b[39m, (\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m), \u001b[39m2\u001b[39m, cv2\u001b[39m.\u001b[39mLINE_AA)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SURYA%20S/gesture/Gesture%20Prediction/Gesture%20Prediction.ipynb#ch0000001?line=106'>107</a>\u001b[0m engine\u001b[39m.\u001b[39msay(className)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/SURYA%20S/gesture/Gesture%20Prediction/Gesture%20Prediction.ipynb#ch0000001?line=107'>108</a>\u001b[0m engine\u001b[39m.\u001b[39;49mrunAndWait()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SURYA%20S/gesture/Gesture%20Prediction/Gesture%20Prediction.ipynb#ch0000001?line=109'>110</a>\u001b[0m \u001b[39m# Show the final output\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SURYA%20S/gesture/Gesture%20Prediction/Gesture%20Prediction.ipynb#ch0000001?line=110'>111</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mOutput\u001b[39m\u001b[39m\"\u001b[39m, frame) \n",
      "File \u001b[1;32mc:\\Users\\SURYA S\\gesture\\lib\\site-packages\\pyttsx3\\engine.py:180\u001b[0m, in \u001b[0;36mEngine.runAndWait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/engine.py?line=177'>178</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inLoop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/engine.py?line=178'>179</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_driverLoop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/engine.py?line=179'>180</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproxy\u001b[39m.\u001b[39;49mrunAndWait()\n",
      "File \u001b[1;32mc:\\Users\\SURYA S\\gesture\\lib\\site-packages\\pyttsx3\\driver.py:192\u001b[0m, in \u001b[0;36mDriverProxy.runAndWait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/driver.py?line=186'>187</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/driver.py?line=187'>188</a>\u001b[0m \u001b[39mCalled by the engine to start an event loop, process all commands in\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/driver.py?line=188'>189</a>\u001b[0m \u001b[39mthe queue at the start of the loop, and then exit the loop.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/driver.py?line=189'>190</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/driver.py?line=190'>191</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_push(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mendLoop, \u001b[39mtuple\u001b[39m())\n\u001b[1;32m--> <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/driver.py?line=191'>192</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_driver\u001b[39m.\u001b[39;49mstartLoop()\n",
      "File \u001b[1;32mc:\\Users\\SURYA S\\gesture\\lib\\site-packages\\pyttsx3\\drivers\\sapi5.py:128\u001b[0m, in \u001b[0;36mSAPI5Driver.startLoop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/drivers/sapi5.py?line=125'>126</a>\u001b[0m     first \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/drivers/sapi5.py?line=126'>127</a>\u001b[0m pythoncom\u001b[39m.\u001b[39mPumpWaitingMessages()\n\u001b[1;32m--> <a href='file:///c%3A/Users/SURYA%20S/gesture/lib/site-packages/pyttsx3/drivers/sapi5.py?line=127'>128</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.05\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "import mediapipe as mp \n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize mediapipe\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the gesture recognizer model\n",
    "model = load_model('./checkpoints/gesture_15_model')\n",
    "\n",
    "# Load class names\n",
    "actions = np.array(['Hello', 'Love You', 'Understand', 'Thanks', 'Some', 'Home', 'name', 'my', 'how', 'Sorry', \"Help me\", \"Yes\", \"No\", \"eat\", \"friend\"])\n",
    "print(actions)\n",
    "\n",
    "sentence = []\n",
    "predictions = []\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.flip(image, 1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*28), (int(prob*100), 90+num*28), colors[1], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Read each frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    x, y, c = frame.shape\n",
    "    frame, result = mediapipe_detection(frame, holistic)\n",
    "    className = ''\n",
    "\n",
    "    # Process the result\n",
    "    if result.left_hand_landmarks or result.right_hand_landmarks:\n",
    "        landmarks = []\n",
    "\n",
    "        lh = [[res.x, res.y] for res in result.left_hand_landmarks.landmark] if result.left_hand_landmarks else np.zeros(21*2).reshape(-1,2).tolist()\n",
    "        rh = [[res.x, res.y] for res in result.right_hand_landmarks.landmark] if result.right_hand_landmarks else np.zeros(21*2).reshape(-1,2).tolist()\n",
    "        \n",
    "        for i in range(len(lh)):\n",
    "            landmarks.append([lh[i][0], lh[i][1], rh[i][0], rh[i][1]])\n",
    "        \n",
    "        # Drawing landmarks on frames\n",
    "        mpDraw.draw_landmarks(frame, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "        mpDraw.draw_landmarks(frame, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "\n",
    "        # Predict gesture\n",
    "        prediction = model.predict([landmarks])\n",
    "        classID = np.argmax(prediction)\n",
    "        predictions.append(classID)\n",
    "        className = actions[classID]\n",
    "\n",
    "        if np.unique(predictions[-20:])[0] == classID: \n",
    "                if prediction[0][classID] > 0.7: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[classID] != sentence[-1]:\n",
    "                            sentence.append(actions[classID])\n",
    "                    else:\n",
    "                        sentence.append(actions[classID])\n",
    "\n",
    "        if len(sentence) > 4: \n",
    "            sentence = sentence[-4:]\n",
    "        \n",
    "\n",
    "        frame = prob_viz(prediction[0], actions, frame, colors)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('r'):\n",
    "            if(len(sentence)!=0):\n",
    "                sentence.pop()\n",
    "            # print(\"Popped: \", sentence)\n",
    "            \n",
    "    # show the prediction on the frame\n",
    "    cv2.rectangle(frame, (0,0), (640, 40), (255, 140, 51), -1)\n",
    "    cv2.putText(frame, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   1, (0,0,255), 2, cv2.LINE_AA)\n",
    "    engine.say(className)\n",
    "    engine.runAndWait()\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04edf71aa90feda7d83661311903b5d99405ed4c425d6368e5339cee55dca3bb"
  },
  "kernelspec": {
   "display_name": "gesture",
   "language": "python",
   "name": "gesture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
